{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S. V. MADHAVA REDDY\n",
      "Creative Programmer\n",
      "@ madhavso2018@gmail.com\n",
      "� +91 9347156120\n",
      "� Vijayawada, India\n",
      "� madhav-somula/\n",
      "SUMMARY\n",
      "Dedicated and results-driven computer science and engineering\n",
      "student with a strong passion for data analysis, machine learning,\n",
      "and web development. Seeking a position to apply my technical\n",
      "skills and innovative mindset to solve complex problems and con-\n",
      "tribute to an organization’s success.\n",
      "EDUCATION\n",
      "B.Tech in Computer Science and Engineering (AIML)\n",
      "NRI Institute of Technology\n",
      "�\n",
      "2020 – 2024\n",
      "� Pothavarappadu\n",
      "• CGPA : 7.9\n",
      "Board of Intermediate Education AP - MPC\n",
      "Sri Chaitanya Educational Institutions\n",
      "�\n",
      "2018 – 2020\n",
      "� Vijayawada\n",
      "• CGPA : 8.4\n",
      "Andhra Pradesh Board of Secondary Education\n",
      "R. K. High School\n",
      "�\n",
      "2017 – 2018\n",
      "� Vijayawada\n",
      "• CGPA : 9.2\n",
      "PROJECTS\n",
      "Data Extractor\n",
      "• TripAdvisor Reviews\n",
      "• EAPCET\n",
      "• Flipkart\n",
      "• Academic Performance\n",
      "Covid-19 Cases Prediction using Machine Learning\n",
      "Developed machine learning models to predict COVID-19 cases,\n",
      "aiding in proactive public health measures.\n",
      "Online Prices Monitor Bot\n",
      "Designed and implemented a real-time price monitoring Telegram\n",
      "bot.\n",
      "SKILLS\n",
      "• Technical Skills:\n",
      "• Programming Languages: Python, Java,\n",
      "C\n",
      "• Data Analysis: Pandas, NumPy\n",
      "• Web Development: HTML, CSS, JavaScript\n",
      "• Database: MySQL, Oracle\n",
      "• Tools:\n",
      "• Data Visualization: Matplotlib, Seaborn\n",
      "• Web Scraping: BeautifulSoup, Selenium\n",
      "• Machine Learning: TensorFlow, scikit-\n",
      "learn\n",
      "• IDEs: Visual Studio Code, PyCharm\n",
      "EXPERIENCE\n",
      "June 2022-September 2022\n",
      "Intern, Blackbucks Engineers PVT LTD\n",
      "� Hyderabad,India\n",
      "Responsibilities:\n",
      "• Collected, organized, and entered critical\n",
      "data.\n",
      "• Coordinated project presentations and\n",
      "created reports for top management.\n",
      "• Utilized Python and Machine Learning for\n",
      "the role.\n",
      "CERTIFICATIONS\n",
      "• Microsoft Certiﬁed\n",
      "DevOps Engineer Expert\n",
      "� Mar – 2023\n",
      "• BlackBucks Engineers PVT LTD\n",
      "Internship Experience Letter\n",
      "� Nov – 2022\n",
      "• IBM Certiﬁcation from Coursera\n",
      "Python Project for Data Engineering\n",
      "� Oct – 2022\n",
      "\n",
      "1967\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as pdf_document:\n",
    "        num_pages = pdf_document.page_count\n",
    "        for page_number in range(num_pages):\n",
    "            page = pdf_document[page_number]\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "pdf_file_path = \"sample.pdf\"\n",
    "pdf_text = read_pdf(pdf_file_path)\n",
    "print(pdf_text)\n",
    "print(len(pdf_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "preprocessed_resumes = preprocess_text(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1593"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26396\\1173080711.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Assuming you've preprocessed the resumes and stored them in 'preprocessed_resumes'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocessed_resumes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreprocessed_resumes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlda_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\madha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[0mcounter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Assuming you've preprocessed the resumes and stored them in 'preprocessed_resumes'\n",
    "dictionary = corpora.Dictionary([preprocessed_resumes.split()])\n",
    "corpus = dictionary.doc2bow([preprocessed_resumes.split()])\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
